---
title: "Preschoolers form conventional pacts with each other to communicate about novel shapes"
bibliography: preschool-tangrams.bib
csl: apa7.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Veronica Boyce (vboyce@stanford.edu)} \\ Department of Psychology, \\Stanford University \And {\large \bf Robert Z. Sparks (bsparks@stanford.edu) }\\ Department of Psychology, \\Stanford University
    \AND {\large \bf Yannick Mofor (yannickm@stanford.edu)} \\ Department of Computer Science, \\ Stanford University \And {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology, \\ Stanford University}

abstract: >
   Learning language requires learning not only the content of language, but also skills that allow us to use language to communicate. Iterated reference games provide a window into such skills, requiring rich communication as participants jointly converge on mutually understandable names for initially novel objects. Some classical experiments with young children are interpreted as showing that 4-5-year old children are incapable of succeeding at iterated reference games. Here, we revisit the question of young children's referential communicative abilities using a simpler, child-friendly paradigm. Across 51 pairs of 4-5-year old children, we find that preschool-aged children were successful in establishing reference. Children were 85% accurate, and they often used descriptions similar to their partner's. These findings suggest that children’s capacity to construct effective referring expressions in novel contexts emerges earlier than once thought, consistent with the view that children show early pragmatic competence in supportive contexts.


keywords: >
    development; iterated reference game; pragmatics; preschoolers
    
output: cogsci2024::cogsci_paper
header-includes: \raggedbottom
#header-includes: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 3, fig.height = 3, fig.crop = F,
  fig.pos = "tb", fig.path = "figs/",
  echo = F, warning = F, cache = F,
  message = F, sanitize = T
)

library(png)
library(grid)
library(ggplot2)
library(xtable)
library(tidyverse)
library(here)
library(brms)
library(rstan)
library(rstanarm)
library(ggthemes)
library(kableExtra)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(theme_bw())

expt_1 <- "data/expt_1"
expt_2 <- "data/expt_2"
mod_loc <- "code/models"
mod_results <- "code/models/summary"
mod_form <- "code/models/formulae"
time_palette <- c("practice" = "grey", "block 1" = "#E41A1C", "block 2" = "#377EB8", "block 3" = "#4DAF4A", "block 4" = "purple") 

# experiment 1 imports
transcript_1 <- read_csv(here(expt_1, "timed_transcript.csv"))
dat_1 <- read_csv(here(expt_1, "clean_data.csv")) |> rename(trial = trialNum)
exclusion_1 <- transcript_1 %>%
  filter(!is.na(description)) %>%
  filter(role == "S") %>%
  select(game, trial) %>%
  unique() %>%
  mutate("S_talked" = 1)

words_1 <- transcript_1 |>
  filter(!is.na(description)) |>
  mutate(words = description |> str_count("\\S+")) |>
  filter(role == "S") |>
  group_by(game, trial) |>
  summarize(words = sum(words))

clean_1 <- dat_1 %>%
  left_join(exclusion_1) %>%
  filter(S_talked == 1) |>
  left_join(words_1) |>
  mutate(type = case_when(
    trial < 2 ~ "practice",
    trial < 6 ~ "block 1",
    trial < 10 ~ "block 2",
    trial < 14 ~ "block 3"
  ))


sims_1 <- read_rds(here(expt_1, "similarities.rds")) |>
  as_tibble() |>
  inner_join(clean_1 |> select(game1 = game, trial1 = trial, target1 = target, speaker1 = speaker)) |>
  inner_join(clean_1 |> select(game2 = game, trial2 = trial, target2 = target, speaker2 = speaker)) |>
  filter(target1 == target2) |>
  filter(trial1 > 1) |>
  filter(trial2 > 1) |>
  mutate(
    trial1 = trial1 + 2,
    trial2 = trial2 + 2
  ) |>
  mutate(block1 = trial1 %/% 4, block2 = trial2 %/% 4) |>
  mutate(
    same_speaker = (speaker1 == speaker2) |> as.numeric(),
    same_game = (game1 == game2) |> as.numeric(),
    later = ifelse(block1 > block2, block1, block2),
    earlier = ifelse(block1 > block2, block2, block1)
  ) |>
  mutate(target = target1)


# expt 2 imports
transcript_2 <- read_csv(here(expt_2, "transcripts.csv"))
link <- read_csv(here(expt_2, "link_transcripts.csv"))

exclude_2 <- transcript_2 |>
  filter(!is.na(echo)) |>
  select(gameId, trial)

data_2 <- read_csv(here(expt_2, "clean_data.csv")) |>
  rename(trial = trialNum) |>
  anti_join(exclude_2)

transcript_2 <- transcript_2 |>
  anti_join(exclude_2) |>
  filter(!is.na(description))

words_2 <- transcript_2 |>
  mutate(words = description |> str_count("\\S+")) |>
  filter(role == "S") |>
  group_by(gameId, gameConfig, trial) |>
  summarize(words = sum(words))

clean_2 <- data_2 |>
  left_join(words_2) |>
  filter(!is.na(words)) |>
  mutate(type = case_when(
    trial < 4 ~ "practice",
    trial < 8 ~ "block 1",
    trial < 12 ~ "block 2",
    trial < 16 ~ "block 3",
    trial < 20 ~ "block 4"
  )) |>
  filter(!is.na(response)) |>
  mutate(correct = as.numeric(correct))

sims_2 <- read_rds(here(expt_2, "similarities.rds")) |>
  as_tibble() |>
  inner_join(clean_2 |> select(game1 = gameId, trial1 = trial, target1 = target, speaker1 = speaker)) |>
  inner_join(clean_2 |> select(game2 = gameId, trial2 = trial, target2 = target, speaker2 = speaker)) |>
  filter(target1 == target2) |>
  mutate(block1 = trial1 %/% 4, block2 = trial2 %/% 4) |>
  mutate(
    same_speaker = (speaker1 == speaker2) |> as.numeric(),
    same_game = (game1 == game2) |> as.numeric(),
    later = ifelse(block1 > block2, block1, block2),
    earlier = ifelse(block1 > block2, block2, block1)
  ) |>
  mutate(target = target1)
```



```{r, eval=F}
library(tidybayes)

save_summary <- function(model) {
  intervals <- gather_draws(model, `b_.*`, regex = T) %>% mean_qi()

  stats <- gather_draws(model, `b_.*`, regex = T) %>%
    mutate(above_0 = ifelse(.value > 0, 1, 0)) %>%
    group_by(.variable) %>%
    summarize(pct_above_0 = mean(above_0)) %>%
    left_join(intervals, by = ".variable") %>%
    mutate(
      lower = .lower,
      upper = .upper,
      Term = str_sub(.variable, 3, -1),
      Estimate = .value
    ) %>%
    select(Term, Estimate, lower, upper)

  stats
}

do_model <- function(path) {
  model <- read_rds(here(mod_loc, path))
  save_summary(model) |> write_rds(here(mod_loc, "summary", path))
  model$formula |> write_rds(here(mod_loc, "formulae", path))
  print(summary(model))
}


mods <- list.files(path = here(mod_loc), pattern = ".*rds") |> walk(~ do_model(.))
```

```{r}
stats <- function(model, row, decimal = 2) {
  model <- model |>
    mutate(
      Estimate = round(Estimate, digits = decimal),
      Lower = round(lower, digits = decimal),
      Upper = round(upper, digits = decimal),
      `Credible Interval` = str_c("[", Lower, ", ", Upper, "]")
    ) |>
    select(Term, Estimate, `Credible Interval`)
  str_c(model[row, 1], ": ", model[row, 2], " ", model[row, 3])
}

stats_text <- function(model, row, decimal = 2) {
  model <- model |>
    mutate(
      Estimate = round(Estimate, digits = decimal) |> formatC(format='f', digits=decimal ),
      Lower = round(lower, digits = decimal) |> formatC(format='f', digits=decimal ),
      Upper = round(upper, digits = decimal) |> formatC(format='f', digits=decimal ),
      `Credible Interval` = str_c("[", Lower, ", ", Upper, "]")
    ) |>
    select(Term, Estimate, `Credible Interval`)
  str_c(model[row, 2], "  ", model[row, 3])
}

form <- function(model_form) {
  dep <- as.character(model_form$formula[2])
  ind <- as.character(model_form$formula[3])

  str_c(dep, " ~ ", ind) |>
    str_replace_all(" ", "") |>
    str_replace_all("\\*", " $\\\\times$ ") |>
    str_replace_all("\\+", "&nbsp;+ ") |>
    str_replace_all("~", "$\\\\sim$ ")
}
```


# Introduction

<!-- intro --> 

Learning a language requires learning not only the content of that language, but also how to use the language to communicate. One test environment for language use is referential communication, the ability to describe a target so an interlocutor can pick it out from a set of possibilities. Adults show sensitivity to both the visual context and their audience during referential communication, calibrating the description they provide to their beliefs about the interlocutor's knowledge state. 

One test of referential communication skill is *iterated reference games* where one describes abstract shapes from an array of images repeatedly to the same partner [@clark1986; @krauss1964; @hawkins2020b; @boyce2024]. Over repetition, features of the initial verbose descriptions are conventionalized as each pair comes to agree on a shared understanding of how to label each image. Success at this task requires mastery of a number of linguistic and communicative skills, including producing adequate initial descriptions, monitoring for comprehension, asking for clarification, and appropriately using the shared conversation history to inform referring expressions in later rounds. 
<!--Indeed, even young children must make use of these skills in order to achieve success in the task. However, there is debate on the developmental trajectories of these communicative skills. --> Studying how children play iterated reference games can provide insight into the developmental trajectory of the ability to produce referential expressions in order to achieve joint understanding. 

A classic study suggests that 4-5-year old preschoolers struggle with child-child referential communication [@glucksberg1966]. In their paradigm, one child was given a set of 6 blocks in a specific order. Their task was to describe the images on the block so their partner could pick out the corresponding block. As children described and selected blocks, they stacked them on pegs. While 4-5-year old children were successful on practice trials with familiar shapes and visual access to each other's blocks, children failed on critical trials where the blocks had drawings of abstract shapes and there was no visual access. Even after multiple rounds with the same images, children were not able to correctly order the blocks. <!--@glucksberg1966 attributed young children's failure to the children’s use of idiosyncratic referring expressions unique to their own experiences, making it difficult for the pair to converge on shared descriptions.-->
Similar experiments with older children indicated a gradual improvement through adolescence both for initial accuracy and for the increase in accuracy across repetitions. Still, even the 9th grade sample was noticeably worse than the sample of adult college students [@glucksberg1967; @krauss1969]. That the task was difficult even among teenagers suggests that the complexity of the stacking paradigm and the large number of potential targets present significant task demands that might prevent accurate measurement of children's abilities. 



```{r interface, fig.env = "figure*", fig.pos = "t!", out.width="\\textwidth", fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Experimental Setup and Procedure. Panel A shows the experimental setup with the teller and guesser across the table from each other. Panel B shows the 4 possible targets; names for targets are for cross-reference with later figures only. Panel C shows the procedure for Experiment 1; within critical blocks targets were ordered randomly. Panel D shows the procedure for Experiment 2. \\label{game}", cache=FALSE}

knitr::include_graphics("diagram.pdf")


# edit diagram at 
# https://docs.google.com/drawings/d/1gio1CNSedECJGrkrNDUi_AydPZcYzESfl4WhZD2l9Bc/edit?usp=sharing
```

More recently, a number of studies have revealed early emerging skills in the preschool years that support the use of referential communication. Preschool-aged children are faster to select a target when it is referred to in a consistent way [@graham2014; @matthews2010] and 6 year olds are more likely to use consistent referential expressions when their partner is consistent [@koymen2014], suggesting that young children are sensitive to the norm of consistent descriptions in cooperative communication. Further, preschool-aged children adapt the informativeness of their referential expressions based on the visual content available to their interlocutor [@matthews2006;@nadig2002;@nilsen2009], showing that children at this age can reason about others’ perspectives to more efficiently communication. 

While children are sensitive to others’ perspectives, they still seem to struggle with appropriately tailoring the specificity of their utterances to the visual context, often resulting in utterances that are under-informative [@matthews2012; @leung2024].  When children must describe one of two very similar images, 4 and 5-year olds struggle to mention the relevant features, although they do better when playing in an interactive game than when not [@grigoroglou2019]. By 5-years old, children are sensitive to utterances that are over- or under-informative, asking for clarification on some under-informative utterances and taking longer to make selections on over-informative utterances [@morisseau2013]. 

A common thread between many studies in referential expression production is that children perform better when the cognitive load of the task is reduced, i.e. when there are fewer possible referents to consider [@abbot-smith2016]. This paired with evidence of emerging communicative skills in young children suggests that tailored referential expression production is possible in children, but it is likely to be masked when the task demands are too high. 

Since @glucksberg1966, few studies have revisited the question of whether children can successfully communicate in an iterated reference game. Recent work shows that 8-10-year olds exhibit adult-like patterns of increasing accuracy, increasing speed, and shorter descriptions across repetitions, but children’s accuracy was still far below adult performance  and highly variable between dyads [@branigan2016]. 4-6-year olds have also been shown to rely on conventionalized gestures with their partner in an iterated reference game where children could only use gestures to communicate [@bohn2019]. Recent work has established that young children can succeed at iterated reference games when playing with their parents [@leung2024]. 4-8 year olds' played an iterated reference game with a parent using a simple, child-friendly tablet-based task. In this task, even 4-year olds had an initial accuracy above 80%, which rose to above 90% in later repetitions [@leung2024]. Together, these findings provide evidence of young children’s ability to successfully communicate about novel referents under the right conditions. 

Given the task demands in @glucksberg1966 and work showing that children can succeed in a less demanding paradigm, we revisit the question of child-child referential communication. In the present study, we re-examine young children's ability to establish effective referring expressions with each other in an iterated reference game using a simplified tablet-based paradigm. Across 51 pairs of 4-5-year old children, we found that preschool-aged children were successful in an iterated reference game, suggesting that children's capacity to construct effective referring expressions in novel contexts emerges earlier than once claimed.


# Experiment 1 

## Methods

<!--(Note: we should talk about whether it makes more sense to do all the methods and then all the results versus the current expt 1 than expt 2 layout.) -->

Our goal for experiment 1 was to test young children's ability to coordinate on descriptions to abstract shapes that their partner could understand. Young children can be very sensitive to task demands and cognitive load (CITATIONS), so we adapted the experimental framework from @leung2024, and further simplified it by reducing the total pool of targets and the number of trials. This experiment was pre-registered at https://osf.io/kcv8j. 

### Participants
4 and 5-year old children were recruited from a university preschool during the school day. Children played with another child from the same class. Experiment 1 was conducted between June and August 2023. Pairs of children were included in analyses if they completed at least 8 of the 12 critical trials. We had 19 complete games and 1 included incomplete game. Of the 40 children, 21 were girls, and the median age was 57 months, with a range of 48-70 months. 

```{r, eval=F}
# demo <- read_csv(here("demographics.csv")) |>
#   filter(included == "x") |>
#   mutate(date_test = mdy(date_test), date_birth = mdy(date_birth)) |>
#   select(date_test, date_birth, gender) |>
#   mutate(
#     age = date_test - date_birth,
#     age = as.numeric(age) / 365.25 * 12
#   ) |> 
#   mutate(expt=ifelse(date_test>ymd("2024-01-01"), "expt2", "expt1")) |> 
#   group_by(expt) |> 
#   summarize(female=sum(gender=="female"),
#             total=n(),
#             median=median(age),
#             min=min(age),
#             max=max(age))
```


### Materials
For the target stimuli, we used four of the ten tangram images from @leung2024, chosen based on visual dissimilarity (Figure \ref{game}B). We coded the matching game using Empirica and hosted it on a lab server [@almaatouq2020]. We then accessed the game on the web on tablets that were locked in a kiosk mode so children could not navigate away from the game.

### Procedure
Once a pair of children agreed to play the game, a research assistant took them to a quiet testing room. Children were introduced to a stuffed animal "Smurfy" who wanted to play a matching game. Children sat across a table from each other, each with a tablet in front of them (Figure \ref{game}A). On each trial, one child was the "teller" and saw a black box around one of two images on their screen and was asked to "say what they saw" in the black box. The "guesser" saw the same two images in a randomized order and tried to select the described image to help Smurfy make a match. When the guesser selected an image, both children received feedback in form of a smiley or frowny face and an audible sound. After each trial, children switched roles. Children passed Smurfy back and forth to help them keep track of whether they were the "guesser" or "teller" on a given trial.

Children completed two warm-up trials with black and white images of familiar shapes, followed by 3 blocks of the 4 target images (Figure \ref{game}C). Targets were randomly paired with another of the critical images as the foil. 

The experimenters running the game did not volunteer descriptions, but they did scaffold the interaction, prompting children to describe the images, and sometimes repeating children's statements. The entire interaction was video-recorded. 

### Data processing

Children's selections and the time to selection were recorded from the experiment software. Children's descriptions were transcribed from the video-recording, using Whisper [@radford2022] for the first pass and then hand-corrected by experimenters. Transcripts were hand-annotated for when each trial started, who said each line, and what referential descriptions were used. 

We excluded trials where the "teller" did not produce a description, or where all description was unintelligible and impossible to transcribe. After exclusions, we had `r clean_2 |> filter(!is.na(repNum)) |> nrow()` trials remaining. 

Statistical analyses were run in brms [@burkner2018] with weakly informative priors. We present the estimate and 95% credible intervals. 


## Results

### Accuracy

```{r fig-accuracy, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="100%", fig.width=5, fig.height=3, fig.cap = "Children's accuracy at selecting the correct target over time. Experiment 1 had 3 blocks (12 total critical trials) and experiment 2 had 4 blocks (16 critical trials). Error bars are bootstrapped 95% CIs with a linear trend line overlaid. \\label{acc}" }
clean_2 |>
  filter(type != "practice") |>
  mutate(trial = trial - 3, expt = "Expt 2") |>
  bind_rows(clean_1 |> filter(type != "practice") |> mutate(trial = trial - 1, expt = "Expt 1")) |>
  ggplot(aes(x = trial, y = correct)) +
    geom_smooth(method = "lm", color = "black") +
  stat_summary(aes(color = type), fun.data = "mean_cl_boot") +
  #stat_summary(fun.data = "mean_cl_boot", geom = "linerange", color = "black") +
  scale_color_manual(values = time_palette) +
  # geom_bar(position = position_dodge(.9), color = "black", fill = "#19ADDE", width = .7) +
  # geom_text(aes(label = percent), hjust = 1.6, color = "white", size = 3) +
  xlab("Trial") +
  ylab("Accuracy") +
  geom_vline(xintercept = 4.5, lty="dashed")+
    geom_vline(xintercept = 8.5, lty="dashed")+
    geom_vline(xintercept = 12.5, lty="dashed")+
  scale_x_continuous(breaks = seq(1, 16, 4), expand = c(0.02, 0.02)) +
  scale_y_continuous(breaks = seq(0, 1, .2), expand = c(0, 0.04)) +
  coord_cartesian(ylim = c(0.3,1))+
  geom_hline(yintercept = .5) +
  facet_grid(expt ~ .) +
  theme(
    legend.position = "none",
    axis.ticks.x=element_blank(),
    axis.text.x=element_blank(),
    panel.grid = element_blank(),
    strip.background = element_blank()
  )
```

```{r, eval=F}
acc_priors <- c(
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)

acc_mod_data_1 <- clean_1 |>
  mutate(
    correct.num = ifelse(correct == TRUE, 1, 0),
    trial.num = trial - 2
  ) |>
  filter(type != "practice")
acc_mod_1 <- brm(correct.num ~ trial.num + (trial.num | game) + (1 | target),
  family = bernoulli(link = "logit"),
  data = acc_mod_data_1,
  file = here(mod_loc, "acc_1.rds"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)
```

```{r}
acc_mod_1 <- read_rds(here(mod_results, "acc_1.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

acc_form <- read_rds(here(mod_form, "acc_1.rds"))
```

Our primary measure of interest was whether children could accurately communicate the intended target. <!-- as prior work is often interpreted as indicating the children at kindergarten age cannot communicate about abstract shapes successfully [@glucksberg1966]. -->  As shown in Figure \ref{acc}, children were above chance in their selections. To test for changes in accuracy over time, we fit a Bayesian mixed effects logistic regression predicting accuracy (`r acc_form |> form()`). Children's accuracy was above chance (Odds Ratio: `r stats_text(acc_mod_1,1)`) and their accuracy slightly increased over the game (OR of one trial later: `r stats_text(acc_mod_1, 2)`). 

### Speed

```{r, eval=F}
speed_priors <- c(
  set_prior("normal(60,100)", class = "Intercept"),
  set_prior("normal(0,20)", class = "b"),
  set_prior("normal(0,20)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)

speed_mod_data_1 <- clean_1 |>
  mutate(
    time.sec = time / 1000,
    trial.num = trial - 2
  ) |>
  filter(type != "practice")

speed_mod_1 <- brm(time.sec ~ trial.num + (trial.num | game) + (1 | target),
  data = speed_mod_data_1,
  file = here(mod_loc, "speed_1.rds"),
  prior = speed_priors,
  control = list(adapt_delta = .95)
)
```

```{r}
speed_mod_1 <- read_rds(here(mod_results, "speed_1.rds"))

speed_form <- read_rds(here(mod_form, "speed_1.rds"))
```

<!--[less important, could drop] --> As another measure of children's performance, we looked at how long each trial took to see if children were getting faster over time. We ran a Bayesian mixed effects linear regression predicting the time to selection in seconds: `r speed_form |> form()`. The first critical trial averaged `r stats_text(speed_mod_1, 1)` seconds, and children got faster over time (`r stats_text(speed_mod_1, 2)` seconds / trial). 

### Description length


```{r, eval=F}
description_priors <- c(
  set_prior("normal(5,10)", class = "Intercept"),
  set_prior("normal(0,5)", class = "b"),
  set_prior("normal(0,5)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)

description_mod_data_1 <- clean_1 |>
  mutate(
    trial.num = trial - 2
  ) |>
  filter(type != "practice")

description_mod_1 <- brm(words ~ trial.num + (trial.num | game) + (1 | target),
  data = description_mod_data_1,
  file = here(mod_loc, "description_1.rds"),
  prior = speed_priors,
  control = list(adapt_delta = .95)
)
```

```{r description, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="100%", fig.width=5, fig.height=3, fig.cap = "Length of referential expression (in words) produced by the speaker each trial. Grey dots are individual data points, colored dots are per trial means with bootstrapped 95% CIs. \\label{length}" }
clean_2 |>
  filter(type != "practice") |>
  mutate(trial = trial - 3, expt = "Expt 2") |>
  bind_rows(clean_1 |> filter(type != "practice") |> mutate(trial = trial - 1, expt = "Expt 1")) |>
  ggplot(aes(x = trial, y = words, color = type)) +
  geom_jitter(alpha = .5, color = "grey") +
  stat_summary(fun.data = "mean_cl_boot") +
  scale_x_continuous(breaks = seq(1, 20, 1), expand = c(0, 0)) +
  scale_color_manual(values = time_palette) +
  geom_smooth(method = "lm", color = "black") +
  xlab("Trial") +
  ylab("Words") +
  facet_grid(expt ~ .) +
  theme(
    legend.position = "none",
    panel.grid = element_blank(),
    strip.background = element_blank()
  )
```




```{r}
description_mod_1 <- read_rds(here(mod_results, "description_1.rds"))

description_form <- read_rds(here(mod_form, "description_1.rds"))
```

In iterated reference games with adults, description lengths usually shorten over repeated references [@clark1986; @hawkins2020b; @boyce2024]. We were curious if children's descriptions would display the same trend, so we ran a Bayesian mixed effects linear regression predicting the number of words in the description the "teller" produced: `r description_form |> form()`. On the first critical trial, descriptions averaged `r stats_text(description_mod_1, 1)` words and description length was relatively stable over time (change of `r stats_text(description_mod_1, 2)` words per trial, Figure \ref{length}). 



### Convergence 

```{r, eval=F}
conv_priors <- c(
  set_prior("normal(.5,.2)", class = "Intercept"),
  set_prior("normal(0,.1)", class = "b"),
  set_prior("normal(0,.05)", class = "sd")
)

sim_across_between_data_1 <- sims_1 |>
  mutate(same_game = case_when(
    game1 == game2 ~ 1,
    T ~ 0
  )) |>
  mutate(same_speaker = ifelse(speaker1 == speaker2, 1, 0))


sim_across_between_mod_1 <- brm(sim ~ same_game + same_speaker + (1 | target),
  data = sim_across_between_data_1,
  file = here(mod_loc, "sim_across_between_1.rds"),
  prior = conv_priors,
  control = list(adapt_delta = .95)
)
```

```{r sims, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="100%", fig.width=5, fig.height=3, fig.cap = "Semantic similarity between referential descriptions from the speaker to the same target under different circumstances. Different games refers to similarities across two speakers from different games, same game to similarities across the two participants in one game, and same speaker to descriptions from the same person in different blocks. Dots are means and lines are bootstrapped 95% CIs.  \\label{sim}" }
sims_2 |>
  mutate(source = case_when(
    speaker1 == speaker2 ~ "same speaker",
    game1 == game2 ~ "same game",
    T ~ "different games"
  )) |>
  mutate(expt = "Expt 2") |>
  bind_rows(sims_1 |>
    mutate(source = case_when(
      speaker1 == speaker2 ~ "same speaker",
      game1 == game2 ~ "same game",
      T ~ "different games"
    )) |> mutate(expt = "Expt 1")) |>
  ggplot(aes(x = source, y = sim, color = target1)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  labs(y = "Cosine similarity", x = "Comparing two utterances from ...", color = "target image") +
  facet_grid(expt ~ .) +
  scale_color_solarized() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank(),
    strip.background = element_blank()
  )
```


```{r}
sim_across_between_1_results <- read_rds(here(mod_results, "sim_across_between_1.rds"))

sim_across_between_form <- read_rds(here(mod_form, "sim_across_between_1.rds"))
```


```{r, include=F}
sims_1 |>
  filter(game1 == game2) |>
  mutate(later = ifelse(block1 > block2, block1, block2), earlier = ifelse(block1 > block2, block2, block1)) |>
  filter(later == 3) |>
  mutate(same_speaker = ifelse(speaker1 == speaker2, "same speaker", "different speaker")) |>
  ggplot(aes(as.character(earlier), sim, color = target1)) +
  geom_point(alpha = .1, position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  facet_wrap(~same_speaker) +
  labs(x = "Earlier block to last block", y = "cosine similarity", color = "target") +
  theme(legend.position = "bottom")
```

```{r include=F}
sims_1 |>
  filter(game1 == game2) |>
  mutate(later = ifelse(block1 > block2, block1, block2), earlier = ifelse(block1 > block2, block2, block1)) |>
  filter(later == earlier + 1) |>
  mutate(same_speaker = ifelse(speaker1 == speaker2, "same speaker", "different speaker")) |>
  ggplot(aes(as.character(earlier), sim, color = target1)) +
  geom_point(alpha = .1, position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  facet_wrap(~same_speaker) +
  labs(x = "Earlier block to *next* block", y = "cosine similarity", color = "target") +
  theme(legend.position = "bottom")
```

```{r, include=F}
sims_1 |>
  filter(game1 != game2) |>
  mutate(later = ifelse(block1 > block2, block1, block2), earlier = ifelse(block1 > block2, block2, block1)) |>
  filter(later == earlier) |>
  ggplot(aes(as.character(earlier), sim, color = target1)) +
  geom_point(alpha = .01, position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  labs(x = "Cross game similarity in block ...", y = "cosine similarity", color = "target") +
  theme(legend.position = "bottom")
```

While description length is used as a proxy for measuring convention formation, better measurements for convergence look at the actual content of the utterances, which does not always track description length [@boyce2024]. Following @boyce2024, we use Sentence-BERT to embed the descriptions in a semantic vector space, and then use the cosine between embeddings as a measure of semantic similarity between descriptions [@reimers2019]. 

We compared the semantic similarities between descriptions of the same target based on who produced the description. Our question was whether the two children within the same game produced descriptions that are more similar to each other than two children in different games. 

Different children in the same game produced more similar descriptions than children in different games, although the descriptions were less similar than descriptions from the same child in different rounds (Figure \ref{sim}). We modeled this as a Bayesian mixed effects linear regression predicting similarity `r sim_across_between_form |> form()`. Utterances were more similar if they came from the same partnership (`r sim_across_between_1_results |> stats_text(2,3)`) and were slightly more similar still if they came from the same person with the partnership (`r sim_across_between_1_results |> stats_text(3,3)`). The big differences in descriptions between games compared to within games is a measure of partner sensitivity.

## Discussion

In the first experiment, we adapted the paradigm of @leung2024 for pairs of children, taking an already simple set up and making it shorter. Our goal was to see if young children were at all able to provide adequate descriptions, so children received a lot of scaffolding around the experimental interaction. Sometimes, this included experimenters echoing children's descriptions which could potentially influence children's responses. In Experiment 2, we repeated the same paradigm, with a tighter experimental script and a larger sample size. 

```{=latex}
\begin{table}
	\caption{Example descriptions children successfully used used to identify different target tangram images in Experiment 2.}
	\label{example}
	\begin{tabular}[t]{p{16em}p{4em}}
		\hline
		%hold
		$\bullet$ person & 		\multirow{6}{*}{\includegraphics[width=.5in]{I1.jpg}}\\
		
		$\bullet$ a person holding a sandwich &\\
		$\bullet$ a people carrying a box of dirt &\\
		$\bullet$ a monster &\\
		$\bullet$ someone holding a plate and giving it to a restaurant and has watermelon &\\
		\hline
		$\bullet$ vampire & \multirow{7}{*}{\includegraphics[width=.5in]{E1.jpg}}\\
		$\bullet$ hopping & \\
		$\bullet$ a person flying &\\
		$\bullet$ a person &\\
		$\bullet$ a kite &\\
		$\bullet$ a triangle with a head on it with feet &\\
				\multicolumn{2}{l}{$\bullet$  somebody skydiving, not in the airplane}\\
		\hline
		$\bullet$ racecar & \multirow{7}{*}{\includegraphics[width=.5in]{D1.jpg}}\\
		$\bullet$ airplane &\\
		$\bullet$ alligator &\\
		$\bullet$ a person fell down &\\
		$\bullet$ a boat &\\
		$\bullet$ a person that's in a race car that has one triangle and two triangles &\\
		\hline
		
		$\bullet$ person & \multirow{4}{*}{\includegraphics[width=.5in]{B1.jpg}}\\
		$\bullet$ a person walking &\\
		$\bullet$ a person looking down &\\
		$\bullet$ a people, but it doesn't have any arms& \\
		\hline
	\end{tabular}
\end{table}

```

# Experiment 2 

## Methods

As Experiment 2 was very similar to Experiment 1, we focus on the changes made compared to Experiment 1. Experiment 2 was pre-registered at https://osf.io/y2dax. 

### Participants
Experiment 2 was run between March and August of 2024, at the same university preschool as experiment 1. No children participated in both experiments. 30 pairs of children completed all 16 critical trials, and 1 pair of children completed between 8 and 16 critical trials. Our target age range was 4 and 5 year olds, but one almost 4-year-old was unintentionally included. Of the 62 children, 30 were girls, and the children had a median age of 56 months, and a range of 45-69 months. 

### Materials
The same 4 critical images were used as in experiment 1, although this time, children saw these images 4 times. In response to some children struggling with the abrupt switch from familiar to non-nameable shapes, we introduced more practice trials for experiment 2. We used a total of 4 practice trials to provide a gradient from familiar shapes to less recognizable, blockier shapes (Figure \ref{game}D). 

### Procedure
The procedure was much the same as Experiment 1(Figure \ref{game}D). We added an initial "bubble popping" exercise to give children practice with how to tap the tablet appropriately (this was an issue for some children in Experiment 1). The experimental script was fully written out and memorized by experimenters so children all received the same instructions. We wrote up contingency statements that the experimenter could use to prompt children who were not giving descriptions or making selections. Experimenters helped with game mechanics such as whose turn it was to tell and who should press the screen to move the game along, but avoided contributing or repeating any content about the images or the descriptions. 

### Data processing
Data was processed in the same way as experiment 1. After excluding trials where children did not give a description or where the experimenter echoed a child's description, we had `r clean_2 |> filter(!is.na(repNum)) |> nrow()` trials total. 

## Results

We report the same set of analyses and model results as in Experiment 1 as well as additional analyses of how the semantic content of children's descriptions changes over time. Children used a wide range of descriptions successfully, and some example descriptions from Experiment 2 are shown in Table \ref{example}. 

### Accuracy

```{r, eval=F}
acc_priors <- c(
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)

acc_mod_data <- clean_2 |>
  mutate(
    correct.num = ifelse(correct == TRUE, 1, 0),
    trial.num = trial - 4
  ) |>
  filter(type != "practice")
acc_mod <- brm(correct.num ~ trial.num + (trial.num | gameId) + (1 | target),
  family = bernoulli(link = "logit"),
  data = acc_mod_data,
  file = here(mod_loc, "acc_2.rds"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)
```

```{r}
acc_mod_2 <- read_rds(here(mod_results, "acc_2.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))
```

In Experiment 2, children's accuracy was above chance (Odds Ratio: `r stats_text(acc_mod_2,1)`) and accuracy was relatively stable over the course of the game (OR of one trial later: `r stats_text(acc_mod_2, 2)`, Figure \ref{acc}). This confirms that children are able to communicate with each other about these abstract shapes. 

### Speed

```{r}
speed_priors <- c(
  set_prior("normal(60,100)", class = "Intercept"),
  set_prior("normal(0,20)", class = "b"),
  set_prior("normal(0,20)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)

speed_mod_data_2 <- clean_2 |>
  mutate(
    time.sec = time / 1000,
    trial.num = trial - 2
  ) |>
  filter(type != "practice")

speed_mod_2 <- brm(time.sec ~ trial.num + (trial.num | gameId) + (1 | target),
  data = speed_mod_data_2,
  file = here(mod_loc, "speed_2.rds"),
  prior = speed_priors,
  control = list(adapt_delta = .95)
)
```

```{r}
speed_mod_2 <- read_rds(here(mod_results, "speed_2.rds"))
```

In Experiment 2, the first critical trial averaged `r stats_text(speed_mod_2, 1)` seconds, and children got faster over time (`r stats_text(speed_mod_2, 2)` seconds / trial). Children were initially faster in Experiment 2 than Experiment 1, possibly due to the increased number of practice trials and pre-training on how to press the screens. 

### Description length

```{r, eval=F}
description_priors <- c(
  set_prior("normal(5,10)", class = "Intercept"),
  set_prior("normal(0,5)", class = "b"),
  set_prior("normal(0,5)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)

description_mod_data_2 <- clean_2 |>
  mutate(
    trial.num = trial - 2
  ) |>
  filter(type != "practice")

description_mod_2 <- brm(words ~ trial.num + (trial.num | gameId) + (1 | target),
  data = description_mod_data_2,
  file = here(mod_loc, "description_2.rds"),
  prior = speed_priors,
  control = list(adapt_delta = .95)
)
```


```{r}
description_mod_2 <- read_rds(here(mod_results, "description_2.rds"))
```

The average length of descriptions on the first trial was `r stats_text(description_mod_2, 1)` words and description length was relatively stable over time (change of `r stats_text(description_mod_2, 2)` words / trial , Figure \ref{length}). This is comparable to Experiment 1, again finding that children produce short utterances without much change in length over time. 










### Convergence 


```{r}
conv_priors <- c(
  set_prior("normal(.5,.2)", class = "Intercept"),
  set_prior("normal(0,.1)", class = "b"),
  set_prior("normal(0,.05)", class = "sd")
)

sim_across_between_data_2 <- sims_2 |>
  mutate(same_game = case_when(
    game1 == game2 ~ 1,
    T ~ 0
  )) |>
  mutate(same_speaker = ifelse(speaker1 == speaker2, 1, 0))


sim_across_between_mod_2 <- brm(sim ~ same_game + same_speaker + (1 | target),
  data = sim_across_between_data_2,
  file = here(mod_loc, "sim_across_between_2.rds"),
  prior = conv_priors,
  control = list(adapt_delta = .95)
)

sim_to_last_data_2 <- sims_2 |>
  filter(game1 == game2) |>
  mutate(later = ifelse(block1 > block2, block1, block2), earlier = ifelse(block1 > block2, block2, block1)) |>
  filter(later == 4) |>
  mutate(
    same_speaker = ifelse(speaker1 == speaker2, 1, 0),
    earlier_block.num = earlier - 1
  )

sim_to_last_mod_2 <- brm(sim ~ earlier_block.num + same_speaker + (1 | game1) + (1 | target),
  data = sim_to_last_data_2,
  file = here(mod_loc, "sim_to_last_2.rds"),
  prior = conv_priors,
  control = list(adapt_delta = .95)
)

sim_to_next_data_2 <- sims_2 |>
  filter(game1 == game2) |>
  mutate(later = ifelse(block1 > block2, block1, block2), earlier = ifelse(block1 > block2, block2, block1)) |>
  filter(later == earlier + 1) |>
  mutate(
    same_speaker = ifelse(speaker1 == speaker2, 1, 0),
    earlier_block.num = earlier - 1
  )

sim_to_next_mod_2 <- brm(sim ~ earlier_block.num + same_speaker + (1 | game1) + (1 | target),
  data = sim_to_next_data_2,
  file = here(mod_loc, "sim_to_next_2.rds"),
  prior = conv_priors,
  control = list(adapt_delta = .95)
)

sim_across_data_2 <- sims_2 |>
  filter(game1 != game2) |>
  filter(block1 == block2) |>
  mutate(block.num = block1 - 1)

sim_across_mod_2 <- brm(sim ~ block.num + (1 | target),
  data = sim_across_data_2,
  file = here(mod_loc, "sim_across_2.rds"),
  prior = conv_priors,
  control = list(adapt_delta = .95)
)
```


```{r, include=F}
sims_2 |>
  filter(game1 == game2) |>
  mutate(later = ifelse(block1 > block2, block1, block2), earlier = ifelse(block1 > block2, block2, block1)) |>
  filter(later == earlier + 1) |>
  mutate(same_speaker = ifelse(speaker1 == speaker2, "same speaker", "different speaker")) |>
  ggplot(aes(as.character(earlier), sim, color = target1)) +
  geom_point(alpha = .1, position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  facet_wrap(~same_speaker) +
  labs(x = "Earlier block to *next* block", y = "cosine similarity", color = "target") +
  theme(legend.position = "bottom")
```

```{r, include=F}
sims_2 |>
  filter(game1 != game2) |>
  mutate(later = ifelse(block1 > block2, block1, block2), earlier = ifelse(block1 > block2, block2, block1)) |>
  filter(later == earlier) |>
  ggplot(aes(as.character(earlier), sim, color = target1)) +
  geom_point(alpha = .01, position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  labs(x = "Cross game similarity in block ...", y = "cosine similarity", color = "target") +
  theme(legend.position = "bottom")
```


```{r}
sim_across_between_2_results <- read_rds(here(mod_results, "sim_across_between_2.rds"))

sim_across_between_form <- read_rds(here(mod_form, "sim_across_between_2.rds"))

sim_across_2_results <- read_rds(here(mod_results, "sim_across_2.rds"))

sim_across_form <- read_rds(here(mod_form, "sim_across_2.rds"))

sim_to_next_2_results <- read_rds(here(mod_results, "sim_to_next_2.rds"))

sim_to_next_form <- read_rds(here(mod_form, "sim_to_next_2.rds"))

sim_to_last_2_results <- read_rds(here(mod_results, "sim_to_last_2.rds"))

sim_to_last_form <- read_rds(here(mod_form, "sim_to_last_2.rds"))
```

<!--To look at semantic distance between utterances, we again operationalized similarity between pairs of utterances as the cosine similarity between their Sentence-BERT embeddings [@reimers2019]. -->

As a coarse comparison, we repeated the analysis from Experiment 1. Utterances were more similar if they came from the same partnership (`r sim_across_between_2_results |> stats_text(2,3)`) and were slightly more similar still if they came from the same person with the partnership (`r sim_across_between_2_results |> stats_text(3,3)`). 

The greater number of trials in Experiment 2 made it possible to look for changes over time that could be indicative of convergence to shared descriptions within a game and divergence between games. 

To look for convergence to shared descriptions within games, we compared the utterances from the first three blocks to the descriptions in the last block using a Bayesian mixed effects linear regression predicting similarity: `r sim_to_last_form |> form()`. Over the first three blocks, descriptions became increasingly similar to the last block description (`r sim_to_last_2_results |> stats_text(2,3)`). Descriptions were more similar to the last block if they came from the same child who gave the description in the last block (`r sim_to_last_2_results |> stats_text(3,3)`). 
<!--Another way to look for convergence is to look at the semantic distance between utterances in adjacent blocks: `r sim_to_next_form |> form()`. --> Although over time descriptions do get more similar to the last block utterance, the semantic distance between adjacent block utterances was relatively constant: `r sim_to_next_2_results |> stats_text(2,3)`. 

As each partnership converged to their shared nicknames, partnerships often diverged from one another as groups focused on distinct aspects of the image. We tested whether descriptions in different games to the same target diverged over time using a Bayesian mixed effects linear regression: `r sim_across_form |> form()`. As the games progressed, descriptions from different games became slightly further apart in semantic space (`r sim_across_2_results |> stats_text(2,3)`). 



```{r sim2, fig.env="figure", fig.pos = "t", fig.align = "center", out.width="100%", fig.width=5, fig.height=3, fig.cap = "Semantic similarity between earlier blocks (1-3) with last block (4) for descriptions to the same image within the same group. SImilarity measured as cosine similarity between S-BERT embeddings of referential descriptions. Heavy dots are means with bootstrapped 95% CIs; light dots are individual values. \\label{tolast}" }
sims_2 |>
  filter(game1 == game2) |>
  mutate(later = ifelse(block1 > block2, block1, block2), earlier = ifelse(block1 > block2, block2, block1)) |>
  filter(later == 4) |>
  mutate(same_speaker = ifelse(speaker1 == speaker2, "same speaker", "different speaker")) |>
  ggplot(aes(as.character(earlier), sim, color = target1)) +
  geom_point(alpha = .1, position = position_dodge(width = .2)) +
  stat_summary(fun.data = "mean_cl_boot", position = position_dodge(width = .2)) +
  facet_wrap(~same_speaker) +
  scale_color_solarized() +
  labs(x = "Earlier block to last block", y = "cosine similarity", color = "target") +
  theme(legend.position = "bottom")
```
# Additional results

## Qualitative examples of interaction structure
Pairs of children varied significantly in how well they could scaffold their own interaction versus needed reminders from experimenters about the structure of the game. In one game, the experimenter provided no input after the practice trials because the children rapidly took turns producing referring expressions and selecting the targets. Their advantage could partially be due to being best friends with each other; all children were paired with another child from the same classroom who they were willing to play with, but friendship levels varied. 

Some children asked clarifying questions of their partner, but knowing how much information to provide was not always consistent. In one game, child A described the figure as "A human", prompting child B to note "There's two humans." Later in the game, child B used the description "A human", leading child A to ask "Which one?", which child B clarified with "The one that is walking". This anecdote illustrates how emerging abilities can be inconsistent -- finding a description inadequate as a guesser does not always translate into providing a more informative description as a teller.

## Pooling across experiments 

```{r, eval=F}

acc_priors <- c(
  set_prior("normal(0,1)", class = "b"),
  set_prior("normal(0,1)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)


acc_mod_data_meta <-  clean_1 |>
  mutate(
    correct.num = ifelse(correct == TRUE, 1, 0),
    trial.num = trial - 2
  ) |>
  filter(type != "practice") |> mutate(gameId=game) |> bind_rows(clean_2 |>
  mutate(
    correct.num = ifelse(correct == TRUE, 1, 0),
    trial.num = trial - 4
  ) |>
  filter(type != "practice"))

# acc_mod_data_meta |> summarize(m=mean(correct.num))

acc_mod_both <- brm(correct.num ~ trial.num + (trial.num | gameId) + (1 | target),
  family = bernoulli(link = "logit"),
  data = acc_mod_data_meta,
  file = here(mod_loc, "acc_meta.rds"),
  prior = acc_priors,
  control = list(adapt_delta = .95)
)

```


```{r, eval=F}
description_priors <- c(
  set_prior("normal(5,10)", class = "Intercept"),
  set_prior("normal(0,5)", class = "b"),
  set_prior("normal(0,5)", class = "sd"),
  set_prior("lkj(1)", class = "cor")
)

description_mod_data_meta <- clean_1 |>
  mutate(
    trial.num = trial - 2
  ) |>
  filter(type != "practice")|> mutate(gameId=game) |> bind_rows(
 clean_2 |>
  mutate(
    trial.num = trial - 2
  ) |>
  filter(type != "practice"))

description_mod_meta <- brm(words ~ trial.num + (trial.num | gameId) + (1 | target),
  data = description_mod_data_meta,
  file = here(mod_loc, "description_meta.rds"),
  prior = speed_priors,
  control = list(adapt_delta = .95)
)

```

```{r, eval=F}
conv_priors <- c(
  set_prior("normal(.5,.2)", class = "Intercept"),
  set_prior("normal(0,.1)", class = "b"),
  set_prior("normal(0,.05)", class = "sd")
)

sim_across_between_data_2 <- sims_2 |>
  mutate(same_game = case_when(
    game1 == game2 ~ 1,
    T ~ 0
  )) |>
  mutate(same_speaker = ifelse(speaker1 == speaker2, 1, 0))

sim_across_between_data_1 <- sims_1 |>
  mutate(same_game = case_when(
    game1 == game2 ~ 1,
    T ~ 0
  )) |>
  mutate(same_speaker = ifelse(speaker1 == speaker2, 1, 0))

sim_across_between_data_meta <- sim_across_between_data_2 |> bind_rows(sim_across_between_data_1)

sim_across_between_mod_meta <- brm(sim ~ same_game + same_speaker + (1 | target),
  data = sim_across_between_data_meta,
  file = here(mod_loc, "sim_across_between_meta.rds"),
  prior = conv_priors,
  control = list(adapt_delta = .95)
)



sim_to_next_data_meta <- sims_2 |> bind_rows(sims_1) |> 
  filter(game1 == game2) |>
  mutate(later = ifelse(block1 > block2, block1, block2), earlier = ifelse(block1 > block2, block2, block1)) |>
  filter(later == earlier + 1) |>
  mutate(
    same_speaker = ifelse(speaker1 == speaker2, 1, 0),
    earlier_block.num = earlier - 1
  )

sim_to_next_mod_meta <- brm(sim ~ earlier_block.num + same_speaker + (1 | game1) + (1 | target),
  data = sim_to_next_data_meta,
  file = here(mod_loc, "sim_to_next_meta.rds"),
  prior = conv_priors,
  control = list(adapt_delta = .95)
)



```



```{r}
acc_mod_meta <- read_rds(here(mod_results, "acc_meta.rds")) |> mutate(across(c(`Estimate`, `lower`, `upper`), ~ exp(.)))

description_mod_meta <- read_rds(here(mod_results, "description_meta.rds"))

sim_across_between_mod_meta <- read_rds(here(mod_results, "sim_across_between_meta.rds"))

sim_to_next_mod_meta <- read_rds(here(mod_results, "sim_to_next_meta.rds"))
```

As the two experiments were similar to one another, we re-ran models pooling the data across the two experiments. 
Pooling the two experiments, children's accuracy was above chance (Odds Ratio: `r stats_text(acc_mod_meta,1)`) and accuracy slightly increased over the course of the game (OR of one trial later: `r stats_text(acc_mod_meta, 2)`). 
The average length of descriptions on the first trial was `r stats_text(description_mod_meta, 1)` words and description length was relatively stable over time (change of `r stats_text(description_mod_meta, 2)` words per trial).

Utterances were more similar if they came from the same partnership (`r sim_across_between_mod_meta |> stats_text(2,3)`) and were slightly more similar still if they came from the same person within the partnership (`r sim_across_between_mod_meta |> stats_text(3,3)`).
<!--Since the two experiments had different length games, we do not pool them in comparisons to the last block, but we can pool them comparing to the next block. The distance between adjacent block utterances is relatively constant: `r sim_to_next_mod_meta |> stats_text(2,3)`. -->

<!-- Query -- I did not include experiment # as a random effect here, and maybe should have? I can rerun if we decide we want this analysis and think expt # as a random effect with game nested within it would be useful. -->

## Relation between accuracy and convergence 


```{r}
# try visual first 

acc_sim_1 <- clean_1 |>
  filter(type != "practice") |> mutate(game1=game) |> 
  mutate(correct=ifelse(correct, 1,0),
         earlier_trial=trial+2) |> 
  select(earlier_trial, target, correct, game1)

acc_sim_2 <- clean_2 |>
  filter(type != "practice") |> select(earlier_trial=trial, target, correct, game1=gameId)

sim_acc_2 <- sims_2 |> 
  filter(game1 == game2) |>
  mutate(later = ifelse(block1 > block2, block1, block2), earlier = ifelse(block1 > block2, block2, block1)) |>
  mutate(earlier_trial = ifelse(trial1 > trial2, trial2, trial1)) |>
  filter(later == earlier + 1) |>
  mutate(
    same_speaker = ifelse(speaker1 == speaker2, 1, 0),
    earlier_block.num = earlier - 1
  ) |> 
  select(earlier_trial, target, game1, earlier, later, sim, earlier_block.num, same_speaker) |> 
  left_join(acc_sim_2)

sim_acc_1 <- sims_1 |> 
  filter(game1 == game2) |>
  mutate(later = ifelse(block1 > block2, block1, block2), earlier = ifelse(block1 > block2, block2, block1)) |>
  mutate(earlier_trial = ifelse(trial1 > trial2, trial2, trial1)) |>
  filter(later == earlier + 1) |>
  mutate(
    same_speaker = ifelse(speaker1 == speaker2, 1, 0),
    earlier_block.num = earlier - 1
  ) |> 
  select(earlier_trial, target, game1, earlier, later, sim, earlier_block.num, same_speaker) |> 
  left_join(acc_sim_1)

#ggplot(sim_acc_1, aes(x=earlier, y=sim, color=as.factor(correct)))+geom_point(alpha=.4, position=position_dodge(.4))+stat_summary()

#ggplot(sim_acc_2, aes(x=earlier, y=sim, color=as.factor(correct)))+geom_point(alpha=.4, position=position_dodge(.4))+stat_summary()

```

```{r, eval=F}
conv_priors <- c(
  set_prior("normal(.5,.2)", class = "Intercept"),
  set_prior("normal(0,.1)", class = "b"),
  set_prior("normal(0,.05)", class = "sd")
)
sim_acc_to_next_data <- sim_acc_1 |> bind_rows(sim_acc_2)
sim_acc_to_next_mod_meta <- brm(sim ~ earlier_block.num*correct + same_speaker*correct + (1 | game1) + (1 | target),
  data = sim_acc_to_next_data,
  file = here(mod_loc, "sim_acc_to_next_meta.rds"),
  prior = conv_priors,
  control = list(adapt_delta = .95)
)

# more stickiness if correct, no clear interactions 
# being correct has a similar size effect to same speaker
```

```{r}
sim_acc_to_next_mod_meta <- read_rds(here(mod_results, "sim_acc_to_next_meta.rds"))
sim_acc_to_next_form <- read_rds(here(mod_form, "sim_acc_to_next_meta.rds"))

```

As a post-hoc measure, we looked at whether descriptions that elicited correct selections had more staying power than descriptions that did not elicit correct selections. Here we operationalized "staying power" as similarity to the next block utterance, and we modeled it with `r form(sim_acc_to_next_form)`. Correct utterances had an increase in similarity with the next block  (`r stats_text(sim_acc_to_next_mod_meta, 2, 3)`) with no substantial interaction with block number or same speaker. Numerically, the size of the boost for being correct was similar to the size of the boost from coming from the same speaker `r stats_text(sim_acc_to_next_mod_meta, 6,3)`. 

# General Discussion

Across 2 experiments and 51 pairs of 4 and 5-year old children, we tested how well children could produce referential expressions that allowed their partner to find a matching abstract shape. Children varied substantially in what sorts of descriptions they produced, but overall accuracy was high (85%), indicating that children were generally able to produce adequate descriptions. Additionally, children's utterances showed signs of converging toward conceptual pacts. Children did not display a reduction in the length of referential expressions over the course of the game.

It is unclear to what extent the uniformly short descriptions are a product of the simplified task or children's behavioral differences from adults. The appropriate length for an initial description depends on how easily describable the target is, as well as how large and close the contrast set is. In this case, the low number of options and relatively easy to describe shapes may have meant long initial descriptions were less necessary. The adult controls in @leung2024 used shorter initial descriptions than adults in studies with larger arrays of harder to distinguish images. However, young children may also choose to provide shorter descriptions than adults would, and young children may accept shorter descriptions as adequate whereas adults may ask for more information to increase their confidence level before making a guess. Especially in light of other work suggesting that conceptual pact formation and reduction in utterance length sometimes decouple in adults [@boyce2024], further empirical work on the factors driving verbosity in reference games is warranted. 

While this task is substantially scaled down relative to measures used for adult competence, it does suggest that the relevant communication skills are present at least in rudimentary form by the end of the preschool years. 

Our task is limited by the non-representative population of children at university nursery schools who participated, and by the limited set of target images. This set of tangram images may be easier to distinguish and refer to than some sets used with adults, leading to overall shorter utterances. We specifically targeted children's abilities to construct referring expressions that can be jointly understood. Children were provided scaffolding around the larger coordination problems of taking turns and talking to their partner.

In the broader picture of language acquisition, there's debate over whether children learn the facts of their language first, followed much later, by pragmatic and communicative competency (CITATIONS), or whether sensitivity to communicative intent is an early emerging skill that develops in parallel with linguistic knowledge and may bootstrap language learning (CITATIONS). The findings in the current work are most consistent with a gradual development of children's communicative and linguistic skills, where the skills emerge early and then are refined over time, as children's cognitive capacities increase. At 4–5 years old, even though their utterances are not fully adult-like in form, children are nonetheless able to use their linguistic skills communicatively. 


<!--# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.-->

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
